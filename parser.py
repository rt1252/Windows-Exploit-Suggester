import re
import requests
from bs4 import BeautifulSoup

def extract_items_from_file(filename):
    # Define regex patterns
    ms_pattern = re.compile(r'\bms\d{2}-\d{3}\b', re.IGNORECASE)
    cve_pattern = re.compile(r'\bcve-\d{4}-\w+\b', re.IGNORECASE)
    
    # Read the file
    with open(filename, 'r') as file:
        content = file.read()
    
    # Find all matches for each pattern
    ms_matches = ms_pattern.findall(content)
    cve_matches = cve_pattern.findall(content)
    
    # Combine the matches and remove duplicates
    all_matches = set(ms_matches + cve_matches)
    
    return all_matches

def extract_matching_urls_from_page(url, patterns):
    # Fetch the page content
    response = requests.get(url)
    content = response.text
    
    # Parse the page content
    soup = BeautifulSoup(content, 'html.parser')
    
    # Find all links in the page
    links = soup.find_all('a', href=True)
    
    # Set to hold unique matching URLs
    matching_urls = set()
    
    # Check each link for the patterns and exclude those containing 'blob'
    for link in links:
        href = link['href']
        if any(pattern in href for pattern in patterns) and 'blob' not in href:
            matching_urls.add(href)
    
    return matching_urls

def save_urls_to_file(urls, filename):
    with open(filename, 'w') as file:
        for url in set(urls):  # Convert list to set to remove duplicates
            # Append https://github.com to each URL
            full_url = f"https://github.com{url}"
            file.write(full_url + '\n')

# Example usage
input_filename = 'output.txt'  # Replace with your file path
page_url = 'https://github.com/SecWiki/windows-kernel-exploits/tree/master'  # Replace with the URL you want to check
output_filename = 'matching_urls.txt'  # Replace with your desired output file name

# Extract patterns from the original file
patterns = extract_items_from_file(input_filename)

# Extract matching URLs from the page source
matching_urls = extract_matching_urls_from_page(page_url, patterns)

# Save the matching URLs to a file
save_urls_to_file(matching_urls, output_filename)
