import re
import requests
from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
from bs4 import BeautifulSoup


def initialize_driver():
    driver = webdriver.Firefox()
    action = ActionChains(driver)
    return driver, action

def extract_items_from_file(filename):
    # Define regex patterns
    ms_pattern = re.compile(r'\bms\d{2}-\d{3}\b', re.IGNORECASE)
    cve_pattern = re.compile(r'\bcve-\d{4}-\d{4,7}\b', re.IGNORECASE)
    
    # Read the file
    with open(filename, 'r') as file:
        content = file.read()
    
    # Find all matches for each pattern
    ms_matches = ms_pattern.findall(content)
    cve_matches = cve_pattern.findall(content)
    
    # Combine the matches and remove duplicates
    all_matches = set(ms_matches + cve_matches)
    
    return all_matches

def extract_matching_urls_from_page(url, patterns):
    try:
        # Fetch the page content
        response = requests.get(url)
        response.raise_for_status()
        content = response.text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching the page: {e}")
        return set()
    
    # Parse the page content
    soup = BeautifulSoup(content, 'html.parser')
    
    # Find all links in the page
    links = soup.find_all('a', href=True)
    
    # Set to hold unique matching URLs
    matching_urls = set()
    
    # Check each link for the patterns and exclude those containing 'blob'
    for link in links:
        href = link['href']
        if any(pattern in href for pattern in patterns) and 'blob' not in href:
            matching_urls.add("http://github.com" + href)
    
    return matching_urls

def main():
    # Example usage
    input_filename = 'output.txt'  # Replace with your file path
    page_url = 'https://github.com/SecWiki/windows-kernel-exploits/tree/master'  # Replace with the URL you want to check

    # Extract patterns from the original file
    patterns = extract_items_from_file(input_filename)

    # Extract matching URLs from the page source
    matching_urls = extract_matching_urls_from_page(page_url, patterns)

    # Initialize WebDriver
    driver, action = initialize_driver()
    
    # Open each matching URL in a new tab
    for url in matching_urls:
        driver.execute_script("window.open(arguments[0], '_blank');", url)
    
    # Switch to the first tab and close it (if desired)
    driver.switch_to.window(driver.window_handles[0])
    
    # Optionally, you can keep the driver open or close it after a delay
    # driver.quit()

if __name__ == "__main__":
    main()
